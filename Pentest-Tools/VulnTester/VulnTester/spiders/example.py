# -*- coding: utf-8 -*-
import scrapy
import re


class GatherDetailsSpider(scrapy.Spider):
    name = 'gather_details'
    greedy = True
    form_regex = re.compile(r"\<form ([\S\s]*?)\<\/form\>")
    forbidden_keys = []

    def __init__(self, domain, **kwargs):
        self.allowed_domains = [f'{domain}']
        self.start_urls = [f'https://{domain}']
        super().__init__(**kwargs)

    def parse(self, response):
        try:
            html = response.body.decode('utf-8')
        except UnicodeDecodeError:
            return
        forms = []
        body_forms = self.form_regex.findall(html)
        forms += [form for form in body_forms]
        yield {'forms': list(set(forms)), 'page': response.request.url}
        if self.greedy:
            links = response.xpath("//a/@href").getall()
            # If there are external links, scrapy will block them
            # because of the allowed_domains setting
            for link in links:
                skip = False
                # for key in self.forbidden_keys:
                #   if key in link:
                #     skip = True
                #     break
                # if skip:
                #   continue
                try:
                    yield scrapy.Request(link, callback=self.parse)
                except ValueError:
                    try:
                        yield response.follow(link, callback=self.parse)
                    except:
                        pass
